{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401d3e58-c7b3-4396-8309-7542ba0985aa",
   "metadata": {},
   "source": [
    "# Image Captioning and Explanation Generation\n",
    "\n",
    "This notebook shows how to generate descriptive captions and query-relevant explanations for images using the BLIP model from Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24c2784-8570-40d1-a3dc-faf63c709070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20da8f-ed0f-49c0-89d3-120fab546343",
   "metadata": {},
   "source": [
    "###  Load Image Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42878e3a-4229-4a2b-956c-4d7d68f60908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path_or_url):\n",
    "    if image_path_or_url.startswith(\"http://\") or image_path_or_url.startswith(\"https://\"):\n",
    "        return Image.open(requests.get(image_path_or_url, stream=True).raw).convert(\"RGB\")\n",
    "    else:\n",
    "        image_path_or_url = '../data/' + image_path_or_url\n",
    "        return Image.open(image_path_or_url).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6867e-3649-4d71-b5f1-1a93cbfa16ec",
   "metadata": {},
   "source": [
    "### Load BLIP Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca288cf-b7d8-4d88-9619-68ee2f255786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RK-Work\\Downloads\\Image-search\\visual-search-system\\backend\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\RK-Work\\Downloads\\Image-search\\visual-search-system\\backend\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cdf2e-b224-4d9b-9fdb-55cb783a8650",
   "metadata": {},
   "source": [
    "### Generate Caption Function (sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8084562-2650-4c09-8278-52d3f395f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path_or_url, query=None):\n",
    "    image = load_image(image_path_or_url)\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    if query:\n",
    "        return f\"{caption}. Relevant to query: '{query}'\"\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4a1831-3ded-42af-8a79-f205ea4a833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RK-Work\\Downloads\\Image-search\\visual-search-system\\backend\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\RK-Work\\Downloads\\Image-search\\visual-search-system\\backend\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a dog wearing a yellow and black shirt\n",
      "Explanation: a dog wearing a yellow and black shirt. Relevant to query: 'dog with hoodie'\n"
     ]
    }
   ],
   "source": [
    "image_path = \"../data/processed/10001.jpg\"  # Or an image URL\n",
    "query = \"dog with hoodie\"\n",
    "\n",
    "caption = generate_caption(image_path)\n",
    "explanation = generate_caption(image_path, query=query)\n",
    "\n",
    "print(\"Caption:\", caption)\n",
    "print(\"Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59f69e-600a-457b-88df-9dfa8f62c067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
