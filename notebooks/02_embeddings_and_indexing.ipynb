{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890431b5-a79d-48ba-ba84-0815bec30996",
   "metadata": {},
   "source": [
    "# Embedding Generation and Vector Indexing (FAISS)\n",
    "\n",
    "This notebook demonstrates how to generate embeddings using OpenCLIP and build a vector index using FAISS for efficient image similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68836c-3600-4319-8b12-b460b610c0fc",
   "metadata": {},
   "source": [
    "### Generate Embeddings (OpenCLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448d216a-f08b-42bc-befc-862cdc217d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_clip import create_model_and_transforms, tokenize\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "model, _, preprocess = create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "tokenizer = tokenize\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = \"../data/processed/10001.jpg\"\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode image\n",
    "with torch.no_grad():\n",
    "    image_embedding = model.encode_image(image)\n",
    "    image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee85f8b-daaf-4c6a-93c1-2f57d830b260",
   "metadata": {},
   "source": [
    "### Build FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd430d65-f72d-4d63-b9e7-a86cb784325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index contains 100 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Let's assume we already have a matrix of embeddings\n",
    "# Simulate with 100 image embeddings\n",
    "num_images = 100\n",
    "dim = image_embedding.shape[-1]\n",
    "image_embeddings = np.random.randn(num_images, dim).astype(\"float32\")\n",
    "image_embeddings = image_embeddings / np.linalg.norm(image_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatIP(dim)  # Inner product (cosine similarity)\n",
    "index.add(image_embeddings)\n",
    "\n",
    "print(\"FAISS index contains\", index.ntotal, \"vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b150e2b-a4b1-4791-8bc9-c097b8e23678",
   "metadata": {},
   "source": [
    "### Query with Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15a465e-741b-4e7c-87db-9dba671be668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode a text query\n",
    "text = \"a photo of a mountain landscape\"\n",
    "text_tokens = tokenizer([text]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embedding = model.encode_text(text_tokens)\n",
    "    text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
    "\n",
    "text_embedding_np = text_embedding.cpu().numpy().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1c6193-584d-4cba-95ab-d12589b9ab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top indices: [[20 62  6 29 31]]\n",
      "Scores: [[0.13009575 0.11412328 0.10218724 0.10147056 0.09993938]]\n"
     ]
    }
   ],
   "source": [
    "# Search top 5 matches\n",
    "k = 5\n",
    "D, I = index.search(text_embedding_np, k)\n",
    "\n",
    "print(\"Top indices:\", I)\n",
    "print(\"Scores:\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648b7cf4-44a0-4e06-8eec-3f4b8276df36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/images/image_20.jpg',\n",
       " 'data/images/image_62.jpg',\n",
       " 'data/images/image_6.jpg',\n",
       " 'data/images/image_29.jpg',\n",
       " 'data/images/image_31.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map retrieved indices back to image metadata\n",
    "image_paths = [f\"data/images/image_{i}.jpg\" for i in range(num_images)]\n",
    "\n",
    "top_results = [image_paths[i] for i in I[0]]\n",
    "top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935164f-f8c9-458c-8792-9c6e982596a9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Used OpenCLIP to encode images and queries into the same embedding space.\n",
    "- Built a FAISS index using cosine similarity (inner product).\n",
    "- Performed efficient top-K image retrieval for a text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed30797-0170-4a7e-969d-7af5dc0a7bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
